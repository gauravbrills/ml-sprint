{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SkLearn cross validaiton https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "**Training, Transforming , Predicting**\n",
    "PCA should be used on training data to find PCs.\n",
    "PCA transform is done on the training data for helping in testing .\n",
    "\n",
    "**K-Fold Cross Validation**:Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).\n",
    "Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n",
    "\n",
    "**Practical Advice for K Fold**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overfit 0.989474\n",
      "optimized 0.689655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_dict = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"rb\") )\n",
    "\n",
    "### first element is our labels, any added elements are predictor\n",
    "### features. Keep this the same for the mini-project, but you'll\n",
    "### have a different feature list when you do the final project.\n",
    "features_list = [\"poi\", \"salary\"]\n",
    "\n",
    "data = featureFormat(data_dict, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(features, labels, test_size=0.30, random_state=42)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "#On full data\n",
    "clf=clf.fit(features, labels)\n",
    "score = clf.score(features, labels)\n",
    "print(\"overfit %f\"%score)\n",
    "clf1 = DecisionTreeClassifier(random_state=0)\n",
    "clf1=clf1.fit(X_train, y_train)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "score = clf1.score(X_test, y_test)\n",
    "print(\"optimized %f\"%score)\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=2)\n",
    "kf.get_n_splits(data)\n",
    "\n",
    "### it's all yours from here forward!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV is a way of systematically working through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance. The beauty is that it can work through many combinations in only a couple extra lines of code.\n",
    "\n",
    "Here's an example from the sklearn documentation:\n",
    "```\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "```\n",
    "Let's break this down line by line.\n",
    "```\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "```\n",
    "A dictionary of the parameters, and the possible values they may take. In this case, they're playing around with the kernel (possible choices are 'linear' and 'rbf'), and C (possible choices are 1 and 10).\n",
    "\n",
    "Then a 'grid' of all the following combinations of values for (kernel, C) are automatically generated:\n",
    "```\n",
    "('rbf', 1)\t('rbf', 10)\n",
    "('linear', 1)\t('linear', 10)\n",
    "```\n",
    "Each is used to train an SVM, and the performance is then assessed using cross-validation.\n",
    "\n",
    "svr = svm.SVC()\n",
    "This looks kind of like creating a classifier, just like we've been doing since the first lesson. But note that the \"clf\" isn't made until the next line--this is just saying what kind of algorithm to use. Another way to think about this is that the \"classifier\" isn't just the algorithm in this case, it's algorithm plus parameter values. Note that there's no monkeying around with the kernel or C; all that is handled in the next line.\n",
    "\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "This is where the first bit of magic happens; the classifier is being created. We pass the algorithm (svr) and the dictionary of parameters to try (parameters) and it generates a grid of parameter combinations to try.\n",
    "```\n",
    "clf.fit(iris.data, iris.target)\n",
    "```\n",
    "And the second bit of magic. The fit function now tries all the parameter combinations, and returns a fitted classifier that's automatically tuned to the optimal parameter combination. You can now access the parameter values via clf.best_params_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params to use are : {'C': 1, 'kernel': 'linear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "D:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "D:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "D:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "D:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "D:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "D:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "D:\\apps\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "iris = datasets.load_iris()\n",
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "GridSearchCV(estimator=svm.SVC(),\n",
    "             param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
    "print(\"Best params to use are : %s\"%clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
